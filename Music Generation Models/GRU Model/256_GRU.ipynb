{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 256 GRU\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v_6ugo2-lX5o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo2zRmK0lT7w"
      },
      "outputs": [],
      "source": [
        "# Install essential packages\n",
        "!pip install pretty_midi\n",
        "!pip install --upgrade tensorflow\n",
        "\n",
        "# Import necessary modules\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, GRU, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "# Suppress warnings and configure TensorFlow logging\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# Verify GPU availability for TensorFlow\n",
        "gpu_count = len(tf.config.list_physical_devices('GPU'))\n",
        "print(\"Available GPUs:\", gpu_count)\n",
        "if gpu_count == 0:\n",
        "    print(\"No GPU detected. Please select a GPU runtime.\")\n",
        "\n",
        "# Activate mixed precision training if supported\n",
        "try:\n",
        "    from tensorflow.keras import mixed_precision\n",
        "    mixed_precision.set_global_policy('mixed_float16')\n",
        "    print(\"Mixed precision training is active.\")\n",
        "except Exception as e:\n",
        "    print(\"Mixed precision training could not be enabled.\")\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Step 1: Acquire the MAESTRO dataset\n",
        "dataset_directory = 'maestro_dataset'\n",
        "if not os.path.exists(dataset_directory):\n",
        "    print(\"\\nInitiating download of the MAESTRO dataset...\")\n",
        "    !wget -q --show-progress https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip\n",
        "\n",
        "    # Step 2: Extract the dataset\n",
        "    print(\"Extracting the MAESTRO dataset...\")\n",
        "    !unzip -q maestro-v3.0.0-midi.zip -d maestro_dataset\n",
        "\n",
        "# Step 3: Preprocess the MIDI files\n",
        "midi_file_paths = glob.glob(os.path.join(dataset_directory, '**', '*.midi'), recursive=True)\n",
        "print(f\"\\nTotal MIDI files located: {len(midi_file_paths)}\")\n",
        "\n",
        "# Optionally limit the number of MIDI files for quicker training.\n",
        "# After many experiments with different MIDI files, the model does not showcase much improvement when more MIDI files are used.\n",
        "midi_file_paths = midi_file_paths[:1200]\n",
        "print(f\"Selected {len(midi_file_paths)} MIDI files for model training.\")\n",
        "\n",
        "# Function to transform MIDI files into note event sequences\n",
        "def extract_note_events(midi_path):\n",
        "    \"\"\"Transforms a MIDI file into a chronological sequence of note events.\"\"\"\n",
        "    try:\n",
        "        midi_object = pretty_midi.PrettyMIDI(midi_path)\n",
        "        note_events = []\n",
        "        for track in midi_object.instruments:\n",
        "            if not track.is_drum:\n",
        "                for note in track.notes:\n",
        "                    note_events.append({\n",
        "                        'note': note.pitch,\n",
        "                        'onset': note.start,\n",
        "                        'offset': note.end,\n",
        "                        'intensity': note.velocity\n",
        "                    })\n",
        "        # Arrange notes by their onset times\n",
        "        note_events.sort(key=lambda event: event['onset'])\n",
        "        return midi_object, note_events\n",
        "    except Exception as error:\n",
        "        print(f\"Failed to process {midi_path}: {error}\")\n",
        "        return None, None\n",
        "\n",
        "# Function to discretize note events based on musical beats\n",
        "def discretize_notes(note_events, midi_object, beat_interval=0.5):\n",
        "    \"\"\"Discretizes note events into fixed intervals aligned with beats.\"\"\"\n",
        "    beats = midi_object.get_beats()\n",
        "    if len(beats) < 2:\n",
        "        # Default to fixed intervals if beat detection fails\n",
        "        beats = np.arange(0, midi_object.get_end_time(), beat_interval)\n",
        "    discretized = []\n",
        "    for i in range(len(beats) - 1):\n",
        "        current_start = beats[i]\n",
        "        current_end = beats[i + 1]\n",
        "        active_notes = [note['note'] for note in note_events\n",
        "                        if note['onset'] < current_end and note['offset'] > current_start]\n",
        "        discretized.append(active_notes)\n",
        "    return discretized\n",
        "\n",
        "# Aggregate all discretized note sequences\n",
        "all_discretized_sequences = []\n",
        "sequence_length_records = []\n",
        "\n",
        "print(\"\\nCommencing MIDI file processing...\")\n",
        "start_timer = time.time()\n",
        "for path in midi_file_paths:\n",
        "    midi_obj, notes = extract_note_events(path)\n",
        "    if notes:\n",
        "        discretized_sequence = discretize_notes(notes, midi_obj)\n",
        "        sequence_length_records.append(len(discretized_sequence))\n",
        "        all_discretized_sequences.append(discretized_sequence)\n",
        "    else:\n",
        "        print(f\"Skipping {path} due to processing issues.\")\n",
        "processing_duration = time.time() - start_timer\n",
        "print(f\"Processing completed in {processing_duration:.2f} seconds.\")\n",
        "\n",
        "print(f\"\\nTotal sequences generated: {len(all_discretized_sequences)}\")\n",
        "print(f\"Average sequence length: {np.mean(sequence_length_records):.2f} steps\")\n",
        "print(f\"Sequence lengths range from {np.min(sequence_length_records)} to {np.max(sequence_length_records)} steps\")\n",
        "\n",
        "# Generate mappings between MIDI pitches and integer indices\n",
        "all_pitch_values = [pitch for seq in all_discretized_sequences for timestep in seq for pitch in timestep]\n",
        "unique_pitches = sorted(set(all_pitch_values))  # Typically 88 for piano\n",
        "\n",
        "pitch_to_index = {pitch: idx for idx, pitch in enumerate(unique_pitches)}\n",
        "index_to_pitch = {idx: pitch for idx, pitch in enumerate(unique_pitches)}\n",
        "total_pitches = len(unique_pitches)  # Expected to be 88\n",
        "\n",
        "print(f\"\\nUnique pitch count: {total_pitches}\")\n",
        "print(f\"Pitch indexing configured for {total_pitches} pitches.\")\n",
        "\n",
        "# Persist pitch mappings for future reference\n",
        "with open('pitch_to_index.pkl', 'wb') as file:\n",
        "    pickle.dump(pitch_to_index, file)\n",
        "with open('index_to_pitch.pkl', 'wb') as file:\n",
        "    pickle.dump(index_to_pitch, file)\n",
        "\n",
        "# Define training hyperparameters\n",
        "past_steps = 64\n",
        "batch_size = 64\n",
        "num_epochs = 40\n",
        "early_stop_patience = 10\n",
        "\n",
        "# Partition the dataset to prevent data leakage\n",
        "training_set, testing_set = train_test_split(all_discretized_sequences, test_size=0.2, random_state=42)\n",
        "training_set, validation_set = train_test_split(training_set, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"Number of training sequences: {len(training_set)}\")\n",
        "print(f\"Number of validation sequences: {len(validation_set)}\")\n",
        "print(f\"Number of testing sequences: {len(testing_set)}\")\n",
        "\n",
        "# Step 4: Define a Data Generator Class\n",
        "class MusicSequenceLoader(Sequence):\n",
        "    def __init__(self, dataset, sequence_len, batch_sz, pitch_count, pitch_map, randomize=True):\n",
        "        \"\"\"\n",
        "        Initializes the data generator for GRU training.\n",
        "\n",
        "        Args:\n",
        "            dataset (list): List of sequences, each containing active pitches per time step.\n",
        "            sequence_len (int): Number of previous time steps to use as input.\n",
        "            batch_sz (int): Size of each data batch.\n",
        "            pitch_count (int): Total number of unique pitches.\n",
        "            pitch_map (dict): Mapping from pitch to index.\n",
        "            randomize (bool): Whether to shuffle data after each epoch.\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.sequence_length = sequence_len\n",
        "        self.batch_size = batch_sz\n",
        "        self.num_pitches = pitch_count\n",
        "        self.pitch_mapping = pitch_map\n",
        "        self.is_random = randomize\n",
        "        self.sample_indices = self._prepare_samples()\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def _prepare_samples(self):\n",
        "        samples = []\n",
        "        for sequence in self.dataset:\n",
        "            if len(sequence) < self.sequence_length + 1:\n",
        "                continue\n",
        "            for i in range(len(sequence) - self.sequence_length):\n",
        "                input_seq = sequence[i:i + self.sequence_length]\n",
        "                target_seq = sequence[i + 1:i + self.sequence_length + 1]\n",
        "                samples.append((input_seq, target_seq))\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.sample_indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_samples = self.sample_indices[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        X = np.zeros((len(batch_samples), self.sequence_length, self.num_pitches), dtype=np.float32)\n",
        "        Y = np.zeros((len(batch_samples), self.sequence_length, self.num_pitches), dtype=np.float32)\n",
        "\n",
        "        for i, (input_seq, target_seq) in enumerate(batch_samples):\n",
        "            for t, pitches in enumerate(input_seq):\n",
        "                for pitch in pitches:\n",
        "                    if pitch in self.pitch_mapping:\n",
        "                        pitch_idx = self.pitch_mapping[pitch]\n",
        "                        if 0 <= pitch_idx < self.num_pitches:\n",
        "                            X[i, t, pitch_idx] = 1.0\n",
        "            for t, pitches in enumerate(target_seq):\n",
        "                for pitch in pitches:\n",
        "                    if pitch in self.pitch_mapping:\n",
        "                        pitch_idx = self.pitch_mapping[pitch]\n",
        "                        if 0 <= pitch_idx < self.num_pitches:\n",
        "                            Y[i, t, pitch_idx] = 1.0\n",
        "        return X, Y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.is_random:\n",
        "            random.shuffle(self.sample_indices)\n",
        "\n",
        "# Step 5: Instantiate Data Generators for Training, Validation, and Testing\n",
        "training_loader = MusicSequenceLoader(training_set, past_steps, batch_size, total_pitches, pitch_to_index)\n",
        "validation_loader = MusicSequenceLoader(validation_set, past_steps, batch_size, total_pitches, pitch_to_index, randomize=False)\n",
        "testing_loader = MusicSequenceLoader(testing_set, past_steps, batch_size, total_pitches, pitch_to_index, randomize=False)\n",
        "\n",
        "# Step 6: Define Dice Loss Function\n",
        "# The following implementation was adapted from Stack Overflow (2024)\n",
        "# Reference: Correct Implementation of Dice Loss in Tensorflow / Keras.\n",
        "# Available at: https://stackoverflow.com/questions/72195156/correct-implementation-of-dice-loss-in-tensorflow-keras\n",
        "\n",
        "def dice_loss(y_true, y_pred, smooth=1):\n",
        "    \"\"\"\n",
        "    Computes the Dice Loss for multi-label classification.\n",
        "\n",
        "    Args:\n",
        "        y_true (tensor): Ground truth binary labels.\n",
        "        y_pred (tensor): Predicted probabilities.\n",
        "        smooth (float): Smoothing factor to prevent division by zero.\n",
        "\n",
        "    Returns:\n",
        "        tensor: Dice loss value.\n",
        "    \"\"\"\n",
        "    y_true_f = tf.reshape(y_true, [-1, total_pitches])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1, total_pitches])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=1)\n",
        "    sum_labels = tf.reduce_sum(y_true_f, axis=1)\n",
        "    sum_preds = tf.reduce_sum(y_pred_f, axis=1)\n",
        "    dice_coeff = (2. * intersection + smooth) / (sum_labels + sum_preds + smooth)\n",
        "    return 1 - tf.reduce_mean(dice_coeff)\n",
        "\n",
        "# Step 7: Build the GRU Model Architecture\n",
        "print(\"\\nAssembling the GRU-based music generation model...\")\n",
        "\n",
        "# Define the input layer with the specified sequence length and number of pitches\n",
        "input_layer = Input(shape=(past_steps, total_pitches), name='music_input')\n",
        "\n",
        "# First Bidirectional GRU Layer\n",
        "first_gru_layer = Bidirectional(\n",
        "    GRU(units=256, return_sequences=True, dropout=0.3), #Mos harro me kthy dropout 0.3\n",
        "    name='bi_gru_layer_1'\n",
        ")(input_layer)\n",
        "\n",
        "# Second Bidirectional GRU Layer\n",
        "second_gru_layer = Bidirectional(\n",
        "    GRU(units=256, return_sequences=True, dropout=0.3),\n",
        "    name='bi_gru_layer_2'\n",
        ")(first_gru_layer)\n",
        "\n",
        "# Third Bidirectional GRU Layer\n",
        "third_gru_layer = Bidirectional(\n",
        "    GRU(units=256, return_sequences=True, dropout=0.3),\n",
        "    name='bi_gru_layer_3'\n",
        ")(second_gru_layer)\n",
        "\n",
        "# Output Dense Layer with sigmoid activation for multi-label prediction\n",
        "output_layer = Dense(total_pitches, activation='sigmoid', name='music_output')(third_gru_layer)\n",
        "\n",
        "# Define the complete model\n",
        "music_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Step 8: Compile the Model with Dice Loss and Built-in Metrics\n",
        "# Define balance factor for handling class imbalance (if needed)\n",
        "balance_factor = 2.0  # Currently not used in Dice Loss\n",
        "\n",
        "# Compile the model with Dice Loss and built-in metrics\n",
        "music_model.compile(\n",
        "    loss=dice_loss,\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    metrics=[\n",
        "        'binary_accuracy',\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "        tf.keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Display the model's architecture summary\n",
        "music_model.summary()\n",
        "\n",
        "# Step 9: Configure Callbacks for Model Training\n",
        "checkpoint_directory = './model_checkpoints'\n",
        "os.makedirs(checkpoint_directory, exist_ok=True)\n",
        "\n",
        "# Checkpoint to save the best model based on validation loss\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_directory, 'best_music_model.keras'),\n",
        "    save_weights_only=False,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Early stopping to halt training when validation loss stops improving\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=early_stop_patience,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Reduce learning rate when a metric has stopped improving\n",
        "lr_reduction_callback = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    verbose=1,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "# Aggregate all callbacks into a list\n",
        "training_callbacks = [checkpoint_callback, early_stopping_callback, lr_reduction_callback]\n",
        "\n",
        "# Step 10: Serialize and Save Training Sequences for Generation\n",
        "def serialize_training_sequences(training_sequences, window_length):\n",
        "    \"\"\"\n",
        "    Saves encoded training sequences to a pickle file for later use in music generation.\n",
        "\n",
        "    Args:\n",
        "        training_sequences (list): List of training sequences with integer-encoded pitches.\n",
        "        window_length (int): The length of the input sequence window.\n",
        "\n",
        "    Saves:\n",
        "        'serialized_train_sequences.pkl': Pickle file containing processed training sequences.\n",
        "    \"\"\"\n",
        "    serialized_sequences = []\n",
        "    for sequence in training_sequences:\n",
        "        encoded_sequence = []\n",
        "        for timestep in sequence:\n",
        "            encoded_timestep = [pitch_idx for pitch_idx in timestep if pitch_idx in pitch_to_index]\n",
        "            if encoded_timestep:\n",
        "                encoded_sequence.append(encoded_timestep)\n",
        "        if len(encoded_sequence) >= window_length:\n",
        "            serialized_sequences.append(encoded_sequence)\n",
        "    with open('serialized_train_sequences.pkl', 'wb') as file:\n",
        "        pickle.dump(serialized_sequences, file)\n",
        "    print(\"Training sequences have been serialized and saved to 'serialized_train_sequences.pkl'.\")\n",
        "\n",
        "# Execute the serialization of training sequences\n",
        "serialize_training_sequences(training_set, past_steps)\n",
        "\n",
        "# Step 11: Begin Model Training\n",
        "print(\"\\nCommencing model training...\")\n",
        "training_history = music_model.fit(\n",
        "    training_loader,\n",
        "    validation_data=validation_loader,\n",
        "    epochs=num_epochs,\n",
        "    callbacks=training_callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 12: Evaluate Model Performance on the Testing Dataset\n",
        "print(\"\\nAssessing model performance on the testing dataset...\")\n",
        "evaluation_results = music_model.evaluate(testing_loader, verbose=1)\n",
        "print(f\"Test Loss: {evaluation_results[0]}\")\n",
        "print(f\"Test Binary Accuracy: {evaluation_results[1]}\")\n",
        "print(f\"Test Precision: {evaluation_results[2]}\")\n",
        "print(f\"Test Recall: {evaluation_results[3]}\")\n",
        "print(f\"Test AUC: {evaluation_results[4]}\")\n",
        "\n",
        "# Step 13: Persist the Final Trained Model for Future Use\n",
        "music_model.save('final_gru_music_model.keras')\n",
        "print(\"The trained model has been saved as 'final_gru_music_model.keras'.\")\n",
        "\n",
        "# Step 14: Function to Visualize Training and Validation Metrics\n",
        "def visualize_training_metrics(history):\n",
        "    \"\"\"\n",
        "    Plots the training and validation metrics over each epoch.\n",
        "\n",
        "    Args:\n",
        "        history (History): Keras History object containing training metrics.\n",
        "    \"\"\"\n",
        "    metrics_to_plot = ['loss', 'binary_accuracy', 'precision', 'recall', 'auc']\n",
        "\n",
        "    plt.figure(figsize=(25, 15))\n",
        "\n",
        "    for idx, metric in enumerate(metrics_to_plot, 1):\n",
        "        plt.subplot(3, 2, idx)\n",
        "        plt.plot(history.history[metric], label=f'Training {metric}')\n",
        "        plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric}')\n",
        "        plt.title(f'{metric.replace(\"_\", \" \").title()} Progress')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel(metric.replace(\"_\", \" \").title())\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate plots for training history\n",
        "visualize_training_metrics(training_history)\n"
      ]
    }
  ]
}