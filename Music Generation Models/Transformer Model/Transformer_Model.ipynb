{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v_6ugo2-lX5o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo2zRmK0lT7w"
      },
      "outputs": [],
      "source": [
        "# Install essential packages if not already present\n",
        "!pip install pretty_midi tensorflow\n",
        "\n",
        "# Import necessary modules\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, LayerNormalization,\n",
        "    MultiHeadAttention, GlobalAveragePooling1D\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import (\n",
        "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "# Suppress warnings and configure TensorFlow logging\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# Verify GPU availability for TensorFlow\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "print(\"Number of GPUs detected:\", len(gpu_devices))\n",
        "if len(gpu_devices) == 0:\n",
        "    print(\"Warning: No GPU detected. Training might be slow.\")\n",
        "\n",
        "# Activate mixed precision training if supported\n",
        "try:\n",
        "    from tensorflow.keras import mixed_precision\n",
        "    mixed_precision.set_global_policy('mixed_float16')\n",
        "    print(\"Mixed precision training is enabled.\")\n",
        "except Exception as e:\n",
        "    print(\"Mixed precision training is not supported on this device or encountered an error.\")\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Step 1: Acquire the MAESTRO dataset\n",
        "dataset_directory = 'maestro_dataset_transformer'\n",
        "if not os.path.exists(dataset_directory):\n",
        "    print(\"\\nInitiating download of the MAESTRO dataset...\")\n",
        "    !wget -q --show-progress https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip\n",
        "\n",
        "    # Step 2: Extract the dataset\n",
        "    print(\"Extracting the MAESTRO dataset...\")\n",
        "    !unzip -q maestro-v3.0.0-midi.zip -d maestro_dataset_transformer\n",
        "\n",
        "# Step 3: Preprocess the MIDI files\n",
        "midi_file_paths = glob.glob(os.path.join(dataset_directory, '**', '*.midi'), recursive=True)\n",
        "print(f\"\\nTotal MIDI files located: {len(midi_file_paths)}\")\n",
        "\n",
        "# Optionally limit the number of MIDI files for quicker training.\n",
        "# It's recommended to use as many MIDI files as possible to improve model performance.\n",
        "midi_file_paths = midi_file_paths[:100]  # Adjust as needed\n",
        "print(f\"Selected {len(midi_file_paths)} MIDI files for model training.\")\n",
        "\n",
        "# Function to transform MIDI files into note event sequences\n",
        "def extract_note_events(midi_path):\n",
        "    \"\"\"Transforms a MIDI file into a chronological sequence of note events.\"\"\"\n",
        "    try:\n",
        "        midi_object = pretty_midi.PrettyMIDI(midi_path)\n",
        "        note_events = []\n",
        "        for track in midi_object.instruments:\n",
        "            if not track.is_drum:\n",
        "                for note in track.notes:\n",
        "                    note_events.append({\n",
        "                        'note': note.pitch,\n",
        "                        'onset': note.start,\n",
        "                        'offset': note.end,\n",
        "                        'intensity': note.velocity\n",
        "                    })\n",
        "        # Arrange notes by their onset times\n",
        "        note_events.sort(key=lambda event: event['onset'])\n",
        "        return midi_object, note_events\n",
        "    except Exception as error:\n",
        "        print(f\"Failed to process {midi_path}: {error}\")\n",
        "        return None, None\n",
        "\n",
        "# Function to discretize note events based on musical beats\n",
        "def discretize_notes(note_events, midi_object, beat_interval=0.5):\n",
        "    \"\"\"Discretizes note events into fixed intervals aligned with beats.\"\"\"\n",
        "    beats = midi_object.get_beats()\n",
        "    if len(beats) < 2:\n",
        "        # Default to fixed intervals if beat detection fails\n",
        "        beats = np.arange(0, midi_object.get_end_time(), beat_interval)\n",
        "    discretized = []\n",
        "    for i in range(len(beats) - 1):\n",
        "        current_start = beats[i]\n",
        "        current_end = beats[i + 1]\n",
        "        active_notes = [note['note'] for note in note_events\n",
        "                        if note['onset'] < current_end and note['offset'] > current_start]\n",
        "        discretized.append(active_notes)\n",
        "    return discretized\n",
        "\n",
        "# Aggregate all discretized note sequences\n",
        "all_discretized_sequences = []\n",
        "sequence_length_records = []\n",
        "\n",
        "print(\"\\nCommencing MIDI file processing...\")\n",
        "start_timer = time.time()\n",
        "for path in midi_file_paths:\n",
        "    midi_obj, notes = extract_note_events(path)\n",
        "    if notes:\n",
        "        discretized_sequence = discretize_notes(notes, midi_obj)\n",
        "        sequence_length_records.append(len(discretized_sequence))\n",
        "        all_discretized_sequences.append(discretized_sequence)\n",
        "    else:\n",
        "        print(f\"Skipping {path} due to processing issues.\")\n",
        "processing_duration = time.time() - start_timer\n",
        "print(f\"Processing completed in {processing_duration:.2f} seconds.\")\n",
        "\n",
        "print(f\"\\nTotal sequences generated: {len(all_discretized_sequences)}\")\n",
        "print(f\"Average sequence length: {np.mean(sequence_length_records):.2f} steps\")\n",
        "print(f\"Sequence lengths range from {np.min(sequence_length_records)} to {np.max(sequence_length_records)} steps\")\n",
        "\n",
        "# Generate mappings between MIDI pitches and integer indices\n",
        "all_pitch_values = [pitch for seq in all_discretized_sequences for timestep in seq for pitch in timestep]\n",
        "unique_pitches = sorted(set(all_pitch_values))  # Typically 88 for piano\n",
        "\n",
        "pitch_to_index = {pitch: idx for idx, pitch in enumerate(unique_pitches)}\n",
        "index_to_pitch = {idx: pitch for idx, pitch in enumerate(unique_pitches)}\n",
        "total_pitches = len(unique_pitches)  # Expected to be 88\n",
        "\n",
        "print(f\"\\nUnique pitch count: {total_pitches}\")\n",
        "print(f\"Pitch indexing configured for {total_pitches} pitches.\")\n",
        "\n",
        "# Persist pitch mappings for future reference\n",
        "with open('transformer_pitch_to_index.pkl', 'wb') as file:\n",
        "    pickle.dump(pitch_to_index, file)\n",
        "with open('transformer_index_to_pitch.pkl', 'wb') as file:\n",
        "    pickle.dump(index_to_pitch, file)\n",
        "\n",
        "# Define training hyperparameters\n",
        "sequence_length = 64  # Should match the generation\n",
        "batch_size = 256\n",
        "num_epochs = 40\n",
        "early_stop_patience = 5\n",
        "\n",
        "# Partition the dataset to prevent data leakage\n",
        "training_set, testing_set = train_test_split(all_discretized_sequences, test_size=0.2, random_state=42)\n",
        "training_set, validation_set = train_test_split(training_set, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"Number of training sequences: {len(training_set)}\")\n",
        "print(f\"Number of validation sequences: {len(validation_set)}\")\n",
        "print(f\"Number of testing sequences: {len(testing_set)}\")\n",
        "\n",
        "# Step 4: Define a Data Generator Class for Transformer\n",
        "class TransformerDataGenerator(Sequence):\n",
        "    def __init__(self, sequences, seq_len, batch_sz, num_pitches, pitch_map, shuffle=True):\n",
        "        \"\"\"\n",
        "        Initializes the data generator for Transformer training.\n",
        "\n",
        "        Args:\n",
        "            sequences (list): List of quantized note sequences.\n",
        "            seq_len (int): Number of previous time steps for input.\n",
        "            batch_sz (int): Size of each batch.\n",
        "            num_pitches (int): Total number of unique pitches.\n",
        "            pitch_map (dict): Mapping from pitch to ID.\n",
        "            shuffle (bool): Whether to shuffle data after each epoch.\n",
        "        \"\"\"\n",
        "        self.sequences = sequences\n",
        "        self.seq_length = seq_len\n",
        "        self.batch_size = batch_sz\n",
        "        self.num_pitches = num_pitches\n",
        "        self.pitch_mapping = pitch_map\n",
        "        self.shuffle = shuffle\n",
        "        self.inputs, self.targets = self._prepare_data()\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        inputs = []\n",
        "        targets = []\n",
        "        for seq in self.sequences:\n",
        "            if len(seq) < self.seq_length + 1:\n",
        "                continue\n",
        "            for i in range(len(seq) - self.seq_length):\n",
        "                input_seq = seq[i:i + self.seq_length]\n",
        "                target_seq = seq[i + 1:i + self.seq_length + 1]  # Shifted by one time step\n",
        "                inputs.append(input_seq)\n",
        "                targets.append(target_seq)\n",
        "        return inputs, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.inputs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_inputs = self.inputs[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        batch_targets = self.targets[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "\n",
        "        # Convert inputs to multi-hot sequences\n",
        "        X = np.zeros((len(batch_inputs), self.seq_length, self.num_pitches), dtype=np.float32)\n",
        "        for i, seq in enumerate(batch_inputs):\n",
        "            for t, pitch_set in enumerate(seq):\n",
        "                for pitch in pitch_set:\n",
        "                    if pitch in self.pitch_mapping:\n",
        "                        X[i, t, self.pitch_mapping[pitch]] = 1.0  # Multi-hot encoding\n",
        "\n",
        "        # Convert targets to multi-hot sequences\n",
        "        Y = np.zeros((len(batch_targets), self.seq_length, self.num_pitches), dtype=np.float32)\n",
        "        for i, seq in enumerate(batch_targets):\n",
        "            for t, pitch_set in enumerate(seq):\n",
        "                for pitch in pitch_set:\n",
        "                    if pitch in self.pitch_mapping:\n",
        "                        Y[i, t, self.pitch_mapping[pitch]] = 1.0\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            combined = list(zip(self.inputs, self.targets))\n",
        "            random.shuffle(combined)\n",
        "            self.inputs, self.targets = zip(*combined)\n",
        "\n",
        "# Step 5: Instantiate Data Generators for Training, Validation, and Testing\n",
        "train_generator = TransformerDataGenerator(training_set, sequence_length, batch_size, total_pitches, pitch_to_index)\n",
        "val_generator = TransformerDataGenerator(validation_set, sequence_length, batch_size, total_pitches, pitch_to_index, shuffle=False)\n",
        "test_generator = TransformerDataGenerator(testing_set, sequence_length, batch_size, total_pitches, pitch_to_index, shuffle=False)\n",
        "\n",
        "# Step 6: Define Dice Loss Function\n",
        "def dice_loss(y_true, y_pred, smooth=1):\n",
        "    \"\"\"\n",
        "    Computes the Dice Loss for multi-label classification.\n",
        "\n",
        "    Args:\n",
        "        y_true (tensor): Ground truth binary labels.\n",
        "        y_pred (tensor): Predicted probabilities.\n",
        "        smooth (float): Smoothing factor to prevent division by zero.\n",
        "\n",
        "    Returns:\n",
        "        tensor: Dice loss value.\n",
        "    \"\"\"\n",
        "    y_true_f = tf.reshape(y_true, [-1, total_pitches])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1, total_pitches])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=1)\n",
        "    sum_labels = tf.reduce_sum(y_true_f, axis=1)\n",
        "    sum_preds = tf.reduce_sum(y_pred_f, axis=1)\n",
        "    dice_coeff = (2. * intersection + smooth) / (sum_labels + sum_preds + smooth)\n",
        "    return 1 - tf.reduce_mean(dice_coeff)\n",
        "\n",
        "# Step 7: Build the Transformer Model Architecture\n",
        "def build_transformer_model(seq_len, num_pitches, embed_dim, num_heads, ff_dim, dropout=0.3):\n",
        "    \"\"\"\n",
        "    Constructs a Transformer-based model for music generation.\n",
        "\n",
        "    Args:\n",
        "        seq_len (int): Length of input sequences.\n",
        "        num_pitches (int): Number of unique pitches.\n",
        "        embed_dim (int): Embedding dimension.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        ff_dim (int): Feed-forward network dimension.\n",
        "        dropout (float): Dropout rate.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Compiled Transformer model.\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=(seq_len, num_pitches), name='input_layer')  # Multi-hot encoded inputs\n",
        "\n",
        "    # Project multi-hot vectors to embedding space\n",
        "    x = Dense(embed_dim, activation='relu', name='input_projection')(inputs)\n",
        "\n",
        "    # Transformer Blocks\n",
        "    for i in range(4):\n",
        "        # Multi-Head Self-Attention\n",
        "        attention_output = MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=dropout, name=f'mha_{i}'\n",
        "        )(x, x)\n",
        "        attention_output = Dropout(dropout)(attention_output)\n",
        "        x = LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
        "\n",
        "        # Feed-Forward Network\n",
        "        ff_output = Dense(ff_dim, activation='relu')(x)\n",
        "        ff_output = Dense(embed_dim)(ff_output)\n",
        "        ff_output = Dropout(dropout)(ff_output)\n",
        "        x = LayerNormalization(epsilon=1e-6)(x + ff_output)\n",
        "\n",
        "    # TimeDistributed Dense Layer to maintain sequence output\n",
        "    x = Dense(embed_dim, activation='relu', name='time_distributed_dense')(x)\n",
        "\n",
        "    # Output Layer with sigmoid activation for multi-label prediction\n",
        "    outputs = Dense(num_pitches, activation='sigmoid', dtype='float32', name='output_layer')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs, name='Transformer_Music_Generator')\n",
        "    return model\n",
        "\n",
        "print(\"\\nBuilding the Transformer-based music generation model...\")\n",
        "embed_dim = 256\n",
        "num_heads = 8\n",
        "ff_dim = 512\n",
        "dropout_rate = 0.5\n",
        "\n",
        "transformer_model = build_transformer_model(sequence_length, total_pitches, embed_dim, num_heads, ff_dim, dropout=dropout_rate)\n",
        "transformer_model.summary()\n",
        "\n",
        "# Step 8: Compile the Model with Dice Loss and Built-in Metrics\n",
        "transformer_model.compile(\n",
        "    loss=dice_loss,  # Using Dice Loss to match GRU model\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    metrics=[\n",
        "        'binary_accuracy',\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "        tf.keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Step 9: Configure Callbacks for Training\n",
        "checkpoint_dir = './transformer_checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "checkpoint_cb = ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_dir, 'best_transformer_model.keras'),\n",
        "    save_weights_only=False,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stop_cb = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=early_stop_patience,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr_cb = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "callbacks = [checkpoint_cb, early_stop_cb, reduce_lr_cb]\n",
        "\n",
        "# Step 10: Serialize and Save Training Sequences for Generation\n",
        "def serialize_training_sequences(sequences, window_size):\n",
        "    \"\"\"\n",
        "    Saves encoded training sequences to a pickle file for later use in music generation.\n",
        "\n",
        "    Args:\n",
        "        sequences (list): List of training sequences with encoded pitch indices.\n",
        "        window_size (int): Length of the input sequence window.\n",
        "\n",
        "    Saves:\n",
        "        'transformer_train_seeds.pkl': Pickle file containing processed training sequences.\n",
        "    \"\"\"\n",
        "    seeds = []\n",
        "    for seq in sequences:\n",
        "        if len(seq) >= window_size:\n",
        "            seeds.append(seq[:window_size])\n",
        "    with open('transformer_train_seeds.pkl', 'wb') as file:\n",
        "        pickle.dump(seeds, file)\n",
        "    print(\"Training seeds have been serialized and saved to 'transformer_train_seeds.pkl'.\")\n",
        "\n",
        "# Execute the serialization of training sequences\n",
        "serialize_training_sequences(training_set, sequence_length)\n",
        "\n",
        "# Step 11: Begin Model Training\n",
        "print(\"\\nCommencing model training...\")\n",
        "training_history = transformer_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=num_epochs,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 12: Evaluate Model Performance on the Testing Dataset\n",
        "print(\"\\nAssessing model performance on the testing dataset...\")\n",
        "evaluation_results = transformer_model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Loss: {evaluation_results[0]}\")\n",
        "print(f\"Test Binary Accuracy: {evaluation_results[1]}\")\n",
        "print(f\"Test Precision: {evaluation_results[2]}\")\n",
        "print(f\"Test Recall: {evaluation_results[3]}\")\n",
        "print(f\"Test AUC: {evaluation_results[4]}\")\n",
        "\n",
        "# Step 13: Persist the Final Trained Model for Future Use\n",
        "transformer_model.save('final_transformer_music_model.keras')\n",
        "print(\"The trained Transformer model has been saved as 'final_transformer_music_model.keras'.\")\n",
        "\n",
        "# Step 14: Function to Visualize Training and Validation Metrics\n",
        "def visualize_training_metrics(history):\n",
        "    \"\"\"\n",
        "    Plots the training and validation metrics over each epoch.\n",
        "\n",
        "    Args:\n",
        "        history (History): Keras History object containing training metrics.\n",
        "    \"\"\"\n",
        "    metrics_to_plot = ['loss', 'binary_accuracy', 'precision', 'recall', 'auc']\n",
        "\n",
        "    plt.figure(figsize=(25, 15))\n",
        "\n",
        "    for idx, metric in enumerate(metrics_to_plot, 1):\n",
        "        plt.subplot(3, 2, idx)\n",
        "        plt.plot(history.history[metric], label=f'Training {metric}')\n",
        "        plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric}')\n",
        "        plt.title(f'{metric.replace(\"_\", \" \").title()} Progress')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel(metric.replace(\"_\", \" \").title())\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate plots for training history\n",
        "visualize_training_metrics(training_history)\n"
      ]
    }
  ]
}